{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is useful to test the model loading and prediction functionalities in isolation. It creates a test folder with a saved model and runs a prediction, which doesn't have any importance since the model is not trained. It is to be used to understand the information flow as well as to debug in the case models don't match or something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "\"\"\"\n",
    "This config file contains the parameters of the best trained model. They must be copied exactly in order to build the exact architecture the weights will subsequently fill.\n",
    "\"\"\"\n",
    "\n",
    "class LSTMBERTConfig(RobertaConfig):\n",
    "    model_type = \"lstm-attn-bert\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Let RobertaConfig initialize all standard attributes\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Store any extra keys as attributes without listing them\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b218b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch import nn\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "\"\"\"\n",
    "This is the model architecture corresponding to the weights saved in the saved_models folder. The layers should match exactly the parameters defined in model_config.py\n",
    "\"\"\"\n",
    "\n",
    "class LSTMBERT(RobertaForSequenceClassification):\n",
    "    def __init__(self, config: LSTMBERTConfig, **kwargs):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "\n",
    "        lstm_input_dim = self.roberta.config.hidden_size + config.visit_time_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            lstm_input_dim,\n",
    "            getattr(config, \"lstm_hidden\"),\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            num_layers=getattr(config, \"lstm_layers\", 1),\n",
    "        )\n",
    "\n",
    "        attn_dim = getattr(config, \"attn_dim\")\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(config.lstm_hidden * 2, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(config.lstm_hidden * 2, config.output_dim)\n",
    "\n",
    "    def forward(self, *args, visit_times: torch.Tensor, **kwargs) -> SequenceClassifierOutput:\n",
    "        \"\"\"\n",
    "        kwargs:\n",
    "            input_ids: (V, S) long\n",
    "            attention_mask: (V, S) long\n",
    "        visit_times: tensor (V, visit_time_dim) float\n",
    "        \n",
    "        where V = number of visits in batch, S = max seq len per visit\n",
    "        \"\"\"\n",
    "        input_ids = kwargs.get(\"input_ids\", args[0] if len(args) > 0 else None)\n",
    "        attention_mask = kwargs.get(\"attention_mask\", args[1] if len(args) > 1 else None)\n",
    "        if input_ids is None or attention_mask is None:\n",
    "            raise ValueError(\"You have to specify input_ids and attention_mask\")\n",
    "        V, S = input_ids.shape\n",
    "        \n",
    "        # Check visit_times shape if needed\n",
    "        if visit_times is None:\n",
    "            raise ValueError(\"You have to provide visit_times tensor\")\n",
    "        if visit_times.shape != (V, self.config.visit_time_dim):\n",
    "            raise ValueError(f\"visit_times shape must be (V, {self.config.visit_time_dim})\")\n",
    "        \n",
    "        # Process each visit through RoBERTa\n",
    "        pooled_visits = []\n",
    "        for i in range(V):\n",
    "            out = self.roberta(\n",
    "                input_ids=input_ids[i:i+1],  # (1, S)\n",
    "                attention_mask=attention_mask[i:i+1],\n",
    "                return_dict=True\n",
    "            )\n",
    "            last_hidden = out.last_hidden_state  # (1, S, hidden)\n",
    "            mask = attention_mask[i:i+1].unsqueeze(-1)  # (1, S, 1)\n",
    "            cls_vec = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)  # (1, hidden)\n",
    "            pooled_visits.append(cls_vec)\n",
    "        \n",
    "        proj = torch.cat(pooled_visits, dim=0)  # (V, hidden)\n",
    "        \n",
    "        # Concatenate visit times if provided\n",
    "        proj = torch.cat([proj, visit_times], dim=-1)  # (V, hidden + visit_time_dim)\n",
    "        \n",
    "        # Add batch dimension for LSTM\n",
    "        proj = proj.unsqueeze(0)  # (1, V, hidden + visit_time_dim)\n",
    "        \n",
    "        # Run LSTM\n",
    "        lstm_out, _ = self.lstm(proj)  # (1, V, 2*lstm_hidden)\n",
    "        lstm_out = lstm_out.squeeze(0)  # (V, 2*lstm_hidden)\n",
    "        \n",
    "        # Attention pooling over visits\n",
    "        scores = self.attn(lstm_out).squeeze(-1)  # (V,)\n",
    "        attention_weights = torch.softmax(scores, dim=0)  # (V,)\n",
    "        \n",
    "        # Weighted sum\n",
    "        pooled = (attention_weights.unsqueeze(-1) * lstm_out).sum(dim=0)  # (2*lstm_hidden,)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled)  # (output_dim,)\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            logits=logits,\n",
    "            attentions=attention_weights.detach().cpu().numpy().tolist()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f0791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test/tokenizer_config.json',\n",
       " 'test/special_tokens_map.json',\n",
       " 'test/vocab.json',\n",
       " 'test/merges.txt',\n",
       " 'test/added_tokens.json',\n",
       " 'test/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "\"\"\"\n",
    "Script to save the model architecture and tokenizer in a folder so that they can be reloaded later for testing purposes. In production, this model shouldn't be used since it's untrained.\n",
    "\"\"\"\n",
    "\n",
    "# load config that must be previously created with the right parameters and the tokenizer (will use the one from the encoder used during training)\n",
    "cfg = LSTMBERTConfig.from_pretrained(\"test\")\n",
    "model = LSTMBERT(cfg)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('PlanTL-GOB-ES/bsc-bio-ehr-es')\n",
    "\n",
    "# save with the right architecture\n",
    "model.save_pretrained('test')\n",
    "tokenizer.save_pretrained('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils for date processing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import cast, List\n",
    "from datetime import date, datetime\n",
    "\n",
    "def date_linear_impute(dates: list[datetime | None]) -> list[date]:\n",
    "    s = pd.to_datetime(pd.Series(dates), errors=\"coerce\")\n",
    "    n = len(s)\n",
    "\n",
    "    # All null -> [1,2,...,n]\n",
    "    if s.dropna().count() < 1: # if only nulls or only one non-null (don't have reference to interpolate)\n",
    "        return [date(2024 + (i // 12), (i % 12) + 1 , 1) for i in range(n)] # by default, do it monthly from Jan 1, 2024\n",
    "\n",
    "    # No nulls -> return same values (as floats)\n",
    "    if all([d is not None for d in dates]):\n",
    "        return cast(List[date], list(dates))\n",
    "\n",
    "    s_int = s.apply(lambda x: x.value if pd.notna(x) else np.nan).astype(\"float64\")  # convert Timestamp -> integer ns since epoch (use float to allow NaN)\n",
    "    # Linear interpolation, allow extrapolation at ends\n",
    "    s_interp = s_int.interpolate(method=\"linear\", limit_direction=\"both\").tolist()\n",
    "    \n",
    "    # there is at least 2 non-nulls, so we can extrapolate\n",
    "    if not dates[0]:\n",
    "        first_valid_index = s.first_valid_index()\n",
    "        assert type (first_valid_index) is int\n",
    "        step = s_interp[first_valid_index + 1] - s_interp[first_valid_index]\n",
    "        for i in range(first_valid_index - 1, -1, -1):\n",
    "            s_interp[i] = s_interp[i + 1] - step\n",
    "\n",
    "    if not dates[-1]:\n",
    "        last_valid_index = s.last_valid_index()\n",
    "        assert type (last_valid_index) is  int\n",
    "        step = s_interp[last_valid_index] - s_interp[last_valid_index - 1]\n",
    "        for i in range(last_valid_index + 1, n):\n",
    "            s_interp[i] = s_interp[i - 1] + step\n",
    "    \n",
    "    dt_series = pd.to_datetime(s_interp)\n",
    "    return [pd.Timestamp(x).date() for x in dt_series.tolist()]\n",
    "\n",
    "def dates_to_log_deltas(case_dates: list[date]) -> list[tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Convert one case's ordered dates into two differnce arrays:\n",
    "      - log_prev: log1p(delta since previous visit)  (first visit -> 0)\n",
    "      - log_start: log1p(delta since first visit)    (first visit -> 0)\n",
    "\n",
    "    Returns:\n",
    "      list of tuples [(log_prev0, log_start0), ...] length == len(case_dates)\n",
    "    \"\"\"\n",
    "    first = case_dates[0]\n",
    "    prev = case_dates[0]\n",
    "\n",
    "    out = []\n",
    "    for dt in case_dates:\n",
    "        # delta from previous (in days, possibly fractional)\n",
    "        delta_prev_seconds = (dt - prev).total_seconds()\n",
    "        delta_prev = delta_prev_seconds / 86400.0 # assuming days are the unit\n",
    "\n",
    "        # delta from first\n",
    "        delta_start_seconds = (dt - first).total_seconds()\n",
    "        delta_start = delta_start_seconds / 86400.0\n",
    "\n",
    "        # first visit: if dt == first then delta_prev may be 0.0, keep that\n",
    "        log_prev = float(torch.log1p(torch.tensor(delta_prev, dtype=torch.float32)).item())\n",
    "        log_start = float(torch.log1p(torch.tensor(delta_start, dtype=torch.float32)).item())\n",
    "\n",
    "        out.append((log_prev, log_start))\n",
    "        prev = dt\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f549585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelClass(ABC):\n",
    "    @abstractmethod\n",
    "    def predict(self, case: list[str], dates: list[str]) -> tuple[float, list[float]]:\n",
    "        pass\n",
    "\n",
    "    def serialize(self, case: list[str], dates: list[str], syn_prob: float, attn_weights: list[float], footer: dict):\n",
    "        \"\"\"This function implements the Common Data Model v2\"\"\"\n",
    "        output = {\n",
    "            \"nlp_output\": {\n",
    "                \"record_metadata\": {\n",
    "                    \"clinical_site_id\": footer['provider_id'],\n",
    "                    \"patient_id\": footer['person_id'],\n",
    "                    \"admission_id\": footer['visit_detail_id'],\n",
    "                    \"record_id\": footer['note_id'],\n",
    "                    \"record_type\": footer['note_type_concept_id'],\n",
    "                    \"record_format\": \"json\",\n",
    "                    \"record_creation_date\": footer['note_datetime'],\n",
    "                    \"record_lastupdate_date\": datetime.now().isoformat(),\n",
    "                    \"record_character_encoding\": \"UTF-8\",\n",
    "                    \"record_extraction_date\": datetime.now().isoformat(),\n",
    "                    \"report_section\": footer['note_title'],\n",
    "                    \"report_language\": \"es\",\n",
    "                    \"deidentified\": \"no\",\n",
    "                    \"deidentification_pipeline_name\": \"\",\n",
    "                    \"deidentification_pipeline_version\": \"\",\n",
    "                    \"case\": case,\n",
    "                    \"dates\": dates,\n",
    "                    \"nlp_processing_date\": datetime.now().isoformat(),\n",
    "                    \"nlp_processing_pipeline_name\": self.__class__.__name__,\n",
    "                    \"nlp_processing_pipeline_version\": \"1.0\",\n",
    "                },\n",
    "                \"syntomatic_probability\": syn_prob,\n",
    "                \"attention_weights\": attn_weights\n",
    "            },\n",
    "            \"nlp_service_info\": {\n",
    "                \"service_app_name\": \"NLP Chagas Prediction\",\n",
    "                \"service_language\": \"es\",\n",
    "                \"service_version\": \"1.0\",\n",
    "                \"service_model\": self.__class__.__name__\n",
    "            }\n",
    "        }\n",
    "        output[\"nlp_output\"][\"processing_success\"] = True\n",
    "        return output\n",
    "\n",
    "\n",
    "class PredictionPipeline(ModelClass):\n",
    "    def __init__(\n",
    "            self,\n",
    "            local_model_path: str\n",
    "        ):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.cfg = LSTMBERTConfig.from_pretrained(local_model_path)\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\n",
    "            local_model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        self.model = LSTMBERT.from_pretrained(local_model_path, config=self.cfg, local_files_only=True)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device) # type: ignore\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            case: list[str],\n",
    "            dates: list[str]\n",
    "        ) -> tuple[float, list[float]]:\n",
    "\n",
    "        inputs = self.tokenizer(case, return_tensors='pt', max_length=self.cfg.max_length, truncation=True, padding='max_length')\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        visit_times_list = self.format_dates(dates)    \n",
    "        visit_times_tensor = torch.tensor(visit_times_list, dtype=torch.float32).to(self.device)\n",
    "        outputs = self.model(inputs['input_ids'], inputs['attention_mask'], visit_times=visit_times_tensor)\n",
    "        syn_prob = outputs.logits.softmax(dim=-1)[1].item()\n",
    "        attn_list = outputs.attentions\n",
    "        return syn_prob, attn_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_dates(dates_str: list[str]) -> list[tuple[float, float]]:\n",
    "        \"\"\"\n",
    "        Convert one case's ordered string dates dates into two differnce arrays:\n",
    "        - log_prev: log1p(delta since previous visit)  (first visit -> 0)\n",
    "        - log_start: log1p(delta since first visit)    (first visit -> 0)\n",
    "\n",
    "        Inputs:\n",
    "        list of string dates in this format: [10Jan2024, 9Apr2024, ...]\n",
    "        Returns:\n",
    "        list of tuples [(log_prev0, log_start0), ...] length == len(dates_str)\n",
    "        \"\"\"\n",
    "        dates = [datetime.strptime(d, \"%d%b%Y\") if d else None for d in dates_str]\n",
    "        dates_imp = date_linear_impute(dates)\n",
    "        return dates_to_log_deltas(dates_imp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5387078523635864, [0.49486151337623596, 0.5051384568214417])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = PredictionPipeline(local_model_path=\"test\") # load model from the test folder created before\n",
    "test_inputs = [\"Paciente con fiebre y dolor de cabeza.\", \"Se observa inflamaci√≥n en las articulaciones.\"]\n",
    "test_visit_dates = ['10Jan2024', '9Apr2024']\n",
    "pipe.predict(test_inputs, test_visit_dates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertfine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
